#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FuncQAè¯„ä¼°è„šæœ¬
ç”¨äºè®¡ç®—funcQAä»»åŠ¡çš„æ­£ç¡®ç‡å’Œé”™è¯¯ç‡

ä½œè€…: AI Assistant
åˆ›å»ºæ—¶é—´: 2024
"""

import json
import argparse
import os
from typing import Dict, List, Tuple


def load_results(file_path: str) -> List[Dict]:
    """
    åŠ è½½JSONLæ ¼å¼çš„ç»“æœæ–‡ä»¶
    
    Args:
        file_path (str): ç»“æœæ–‡ä»¶è·¯å¾„
        
    Returns:
        List[Dict]: ç»“æœæ•°æ®åˆ—è¡¨
        
    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
        json.JSONDecodeError: JSONæ ¼å¼é”™è¯¯
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    results = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                results.append(data)
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONæ ¼å¼é”™è¯¯: {e}")
                continue
    
    return results


def calculate_metrics(results: List[Dict]) -> Dict[str, float]:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    Args:
        results (List[Dict]): ç»“æœæ•°æ®åˆ—è¡¨
        
    Returns:
        Dict[str, float]: åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸
    """
    total_count = len(results)
    if total_count == 0:
        return {
            'total': 0,
            'correct': 0,
            'incorrect': 0,
            'accuracy': 0.0,
            'error_rate': 0.0
        }
    
    correct_count = 0
    incorrect_count = 0
    
    for result in results:
        check_index = result.get('check_index', 0)
        if check_index == 1:
            correct_count += 1
        elif check_index == -1:
            incorrect_count += 1
        # check_indexä¸º0æˆ–å…¶ä»–å€¼çš„æƒ…å†µä¸è®¡å…¥æ­£ç¡®æˆ–é”™è¯¯
    
    # è®¡ç®—æ­£ç¡®ç‡å’Œé”™è¯¯ç‡
    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0.0
    error_rate = (incorrect_count / total_count) * 100 if total_count > 0 else 0.0
    
    return {
        'total': total_count,
        'correct': correct_count,
        'incorrect': incorrect_count,
        'accuracy': accuracy,
        'error_rate': error_rate
    }


def analyze_detailed_results(results: List[Dict]) -> Dict[str, List[Dict]]:
    """
    åˆ†æè¯¦ç»†ç»“æœï¼ŒæŒ‰æ­£ç¡®æ€§åˆ†ç±»
    
    Args:
        results (List[Dict]): ç»“æœæ•°æ®åˆ—è¡¨
        
    Returns:
        Dict[str, List[Dict]]: æŒ‰æ­£ç¡®æ€§åˆ†ç±»çš„ç»“æœ
    """
    correct_results = []
    incorrect_results = []
    uncertain_results = []
    
    for result in results:
        check_index = result.get('check_index', 0)
        simplified_result = {
            'ID': result.get('ID', 'N/A'),
            'question': result.get('question', 'N/A')[:100] + '...' if len(result.get('question', '')) > 100 else result.get('question', 'N/A'),
            'check_index': check_index
        }
        
        if check_index == 1:
            correct_results.append(simplified_result)
        elif check_index == -1:
            incorrect_results.append(simplified_result)
        else:
            uncertain_results.append(simplified_result)
    
    return {
        'correct': correct_results,
        'incorrect': incorrect_results,
        'uncertain': uncertain_results
    }


def print_evaluation_report(metrics: Dict[str, float], detailed_results: Dict[str, List[Dict]], show_details: bool = False):
    """
    æ‰“å°è¯„ä¼°æŠ¥å‘Š
    
    Args:
        metrics (Dict[str, float]): è¯„ä¼°æŒ‡æ ‡
        detailed_results (Dict[str, List[Dict]]): è¯¦ç»†ç»“æœåˆ†æ
        show_details (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print("\n" + "="*60)
    print("                FuncQA è¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»é¢˜æ•°:     {metrics['total']:>6}")
    print(f"   æ­£ç¡®æ•°:     {metrics['correct']:>6}")
    print(f"   é”™è¯¯æ•°:     {metrics['incorrect']:>6}")
    print(f"   æœªç¡®å®š:     {len(detailed_results['uncertain']):>6}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ­£ç¡®ç‡:     {metrics['accuracy']:>6.2f}%")
    print(f"   é”™è¯¯ç‡:     {metrics['error_rate']:>6.2f}%")
    print(f"   æœªç¡®å®šç‡:   {(len(detailed_results['uncertain'])/metrics['total']*100 if metrics['total'] > 0 else 0):>6.2f}%")
    
    # è¯¦ç»†ä¿¡æ¯
    if show_details:
        print(f"\nğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
        
        if detailed_results['incorrect']:
            print(f"\nâŒ é”™è¯¯é¢˜ç›® ({len(detailed_results['incorrect'])}é¢˜):")
            for i, result in enumerate(detailed_results['incorrect'][:10], 1):  # åªæ˜¾ç¤ºå‰10é¢˜
                print(f"   {i:2d}. ID:{result['ID']} - {result['question']}")
            if len(detailed_results['incorrect']) > 10:
                print(f"   ... è¿˜æœ‰ {len(detailed_results['incorrect']) - 10} é¢˜")
        
        if detailed_results['uncertain']:
            print(f"\nâ“ æœªç¡®å®šé¢˜ç›® ({len(detailed_results['uncertain'])}é¢˜):")
            for i, result in enumerate(detailed_results['uncertain'][:5], 1):  # åªæ˜¾ç¤ºå‰5é¢˜
                print(f"   {i:2d}. ID:{result['ID']} - {result['question']}")
            if len(detailed_results['uncertain']) > 5:
                print(f"   ... è¿˜æœ‰ {len(detailed_results['uncertain']) - 5} é¢˜")
    
    print("\n" + "="*60)


def save_report_to_file(metrics: Dict[str, float], detailed_results: Dict[str, List[Dict]], output_file: str):
    """
    å°†è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        metrics (Dict[str, float]): è¯„ä¼°æŒ‡æ ‡
        detailed_results (Dict[str, List[Dict]]): è¯¦ç»†ç»“æœåˆ†æ
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    report_data = {
        'summary': {
            'total_questions': metrics['total'],
            'correct_answers': metrics['correct'],
            'incorrect_answers': metrics['incorrect'],
            'uncertain_answers': len(detailed_results['uncertain']),
            'accuracy_percentage': round(metrics['accuracy'], 2),
            'error_rate_percentage': round(metrics['error_rate'], 2),
            'uncertain_rate_percentage': round(len(detailed_results['uncertain'])/metrics['total']*100 if metrics['total'] > 0 else 0, 2)
        },
        'detailed_results': detailed_results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(
        description='FuncQAè¯„ä¼°è„šæœ¬ - è®¡ç®—æ­£ç¡®ç‡å’Œé”™è¯¯ç‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python evaluate_funcQA.py FuncQA_funcqa_mh_deepseek-chat_easytool.jsonl
  python evaluate_funcQA.py results.jsonl --details
  python evaluate_funcQA.py results.jsonl --output report.json
        """
    )
    
    parser.add_argument(
        'input_file',
        help='è¾“å…¥çš„JSONLç»“æœæ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--details', '-d',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†çš„é”™è¯¯å’Œæœªç¡®å®šé¢˜ç›®ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='è¾“å‡ºè¯¦ç»†æŠ¥å‘Šåˆ°JSONæ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½ç»“æœæ–‡ä»¶
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶: {args.input_file}")
        results = load_results(args.input_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(results)} æ¡è®°å½•")
        
        # è®¡ç®—æŒ‡æ ‡
        print("ğŸ”¢ æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = calculate_metrics(results)
        
        # åˆ†æè¯¦ç»†ç»“æœ
        detailed_results = analyze_detailed_results(results)
        
        # æ‰“å°æŠ¥å‘Š
        print_evaluation_report(metrics, detailed_results, args.details)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        if args.output:
            save_report_to_file(metrics, detailed_results, args.output)
        
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())